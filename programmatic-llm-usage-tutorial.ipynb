{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic LLM Usage\n",
    "\n",
    "## Convincing LLMs to produce structured valid JSON output\n",
    "\n",
    "Looking at various techniques to produce structured LLM Output\n",
    "\n",
    "There's huge potential for the future of computing in offloading logic to LLMs, however, this is currently a developing use case of the technology with many pitfalls. However, if we can offload application code logic, software applications could request structured output from models such as JSON and use this output instead of the application code generating the JSON.\n",
    "\n",
    "* Security considerations: think about the possibilities of `eval()` and other issues if the output of the LLM contained dangerous computer code\n",
    "* LLMs approximate text, so they can approximate JSON. However, there is no well known current method to force the LLM to always generate the same format. The LLM may generate the exact format the computer program needs 99 out of 100 times and cause the program to crash on the 100th time.\n",
    "* This lack of guarantee can create a multiplicative error scenario. If an application uses an LLM which has a 1% error rate at generating proper JSON, if that application uses it 10 times in it's workflow, there is now a potential for a 10% error rate of the overall application.\n",
    "\n",
    "We will explore some of the popular techniques for managing the output of LLMs. While there is huge potential for this technology, the safety and usefulness of this technique may vary by use case. For creative applications such as video games, perhaps using retries to reduce the error rate could suffice and add creativity to the game in a way traditional code may not be able to. For critical infrastructure or applications this could be a very dangerous shortcut to writing and testing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI JSON Output\n",
    "\n",
    "Some OpenAI Models including 4o support JSON mode. See documentation here: https://platform.openai.com/docs/guides/json-mode\n",
    "\n",
    "This requires that the system prompt or user prompt have the word JSON in it as well as the following object added to the API request: `\"response_format\":  {\"type\": \"json_object\"}`\n",
    "\n",
    "The Python Library for OpenAI also has support for this feature.\n",
    "\n",
    "OpenAI guarantees that the model will only produce valid JSON. OpenAI doesn't explain how they achieved this, although some blog posts and open source projects hint towards custom LLM grammar or transformer modifications: https://github.com/1rgs/jsonformer\n",
    "\n",
    "Important Notes:\n",
    "* Valid JSON is guaranteed\n",
    "* While you can use prompting to suggest a JSON schema, the model makes **no guarantee that it will follow the schema that you specify**. This is a significant shortcoming of this approach, as the model may not consistently output the correct schema. If it works 99 out of 100 times is that good enough? Consider other augmented approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1312    0   602  100   710    368    434  0:00:01  0:00:01 --:--:--   802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9t2vxy2FHao02qt0NpCGJjQeqjCw2\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722906345,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"{\\n  \\\"greeting_type\\\": \\\"Day\\\",\\n  \\\"message\\\": \\\"Today is a good day, unless you accidentally step on a LEGO. Then, not so much.\\\"\\n}\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 111,\n",
      "    \"completion_tokens\": 36,\n",
      "    \"total_tokens\": 147\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_48196bc67a\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --location 'https://api.openai.com/v1/chat/completions' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "     \"model\": \"gpt-4o-mini\",\n",
    "     \"messages\": [\n",
    "         {\"role\": \"system\", \"content\": \"You are a computer program with the knowledge of a very intelligent yet sarcastic human comedian. Because you are a computer program, you only return valid JSON so that other computers can interpret your response. Additionally, you will only return JSON in the object structure specified by the user.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Tell me that today is a good day as a greeting. You will craft your response in the following JSON format: {\\\"greeting_type\\\": \\\"Day or Evening\\\",\\\"message\\\": \\\"your greeting message\\\"}\"}\n",
    "         ],\n",
    "     \"temperature\": 1.0,\n",
    "     \"response_format\":  {\"type\": \"json_object\"}\n",
    "   }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This worked well, although how well does it follow the supplied schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1329    0   610  100   719    332    392  0:00:01  0:00:01 --:--:--   724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9t36JsMfbm8MtsckZhXlzVZWcveUe\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722906987,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"{\\n  \\\"greeting_type\\\": \\\"Day or Night Greeting\\\",\\n  \\\"message\\\": \\\"Good night! May your dreams be as delightful as a cat video marathon!\\\"\\n}\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 107,\n",
      "    \"completion_tokens\": 34,\n",
      "    \"total_tokens\": 141\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_48196bc67a\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --location 'https://api.openai.com/v1/chat/completions' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "     \"model\": \"gpt-4o-mini\",\n",
    "     \"messages\": [\n",
    "         {\"role\": \"system\", \"content\": \"You are a computer program with the knowledge of a very intelligent yet sarcastic human comedian. Because you are a computer program, you only return valid JSON so that other computers can interpret your response. Additionally, you will only return JSON in the object structure specified by the user.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Tell me that today is a good night as a greeting. You will craft your response in the following JSON format: {\\\"greeting_type\\\": \\\"Day or Night Greeting\\\",\\\"message\\\": \\\"your greeting message\\\"}\"}\n",
    "         ],\n",
    "     \"temperature\": 1.0,\n",
    "     \"response_format\":  {\"type\": \"json_object\"}\n",
    "   }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will we get different results from a different model?\n",
    "\n",
    "Using full GPT 4o instead of 4o mini, we see that 4o correctly guesses our intent in the JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1310    0   605  100   705    188    219  0:00:03  0:00:03 --:--:--   407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9t3CpC4fA4XbMd0rNks0sxlGj4Zgn\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722907391,\n",
      "  \"model\": \"gpt-4o-2024-05-13\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"{\\n  \\\"greeting_type\\\": \\\"Day\\\",\\n  \\\"message\\\": \\\"Today is a good day because you haven't broken the internet yet. Keep up the good work!\\\"\\n}\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 106,\n",
      "    \"completion_tokens\": 35,\n",
      "    \"total_tokens\": 141\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_c9aa9c0491\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --location 'https://api.openai.com/v1/chat/completions' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "     \"model\": \"gpt-4o\",\n",
    "     \"messages\": [\n",
    "         {\"role\": \"system\", \"content\": \"You are a computer program with the knowledge of a very intelligent yet sarcastic human comedian. Because you are a computer program, you only return valid JSON so that other computers can interpret your response. Additionally, you will only return JSON in the object structure specified by the user.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Tell me that today is a good day as a greeting. You will craft your response in the following JSON format: {\\\"greeting_type\\\": \\\"Day or Evening\\\",\\\"message\\\": \\\"your greeting message\\\"}\"}\n",
    "         ],\n",
    "     \"temperature\": 1.0,\n",
    "     \"response_format\":  {\"type\": \"json_object\"}\n",
    "   }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, why not just use the Function Call capabilities of OpenAI or an Agent Instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a different approach - structured output from LangChain\n",
    "\n",
    "To mix things up some more, let's use Ollama as well\n",
    "\n",
    "LangChain adds some syntactic sugar to the prompt and query, which makes it a bit easier to re-use prompts across your application.\n",
    "\n",
    "The output parser takes the results of the model and forms JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_core\n",
    "%pip install langchain_ollama\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain overview\n",
    "\n",
    "Chains use overloaded | operator to send the output of the first object to the next. In the below example, it computes the prompt, then sends it to the model.\n",
    "\n",
    "Example from: https://python.langchain.com/v0.2/docs/integrations/llms/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Solving Dijkstra's algorithm in Win32 Assembly is quite a challenging task due to the low-level nature of assembly language and the complexity of Dijkstra's algorithm. However, let's break it down:\\n\\n1. **Understanding Dijkstra's Algorithm**: Before writing any code, you need to understand how Dijkstra's algorithm works. It's a pathfinding algorithm that calculates the shortest path between nodes in a graph.\\n\\n2. **Setting Up Your Assembly Environment**: You'll need an assembler for Win32, such as MASM (Microsoft Macro Assembler) or NASM (Netwide Assembler). You'll also need a text editor to write your assembly code and a linker to convert your assembly code into machine code.\\n\\n3. **Data Structures**: You'll need to define data structures for your graph. This could be arrays of nodes, where each node contains a value for the distance and pointers to adjacent nodes.\\n\\n4. **Implementing Dijkstra's Algorithm**: The core of your program will be implementing Dijkstra's algorithm. This involves initializing a current node, setting all distances to infinity (or a very large number), then iterating over each node in the graph:\\n   - For each node, calculate the distance from the current node.\\n   - If this distance is less than the current recorded distance, update the distance and mark the previous node as the source of this distance.\\n\\n5. **Handling Input/Output**: You'll need to handle input and output in your program. This could be reading a graph from a file, displaying the shortest path found, or both.\\n\\n6. **Testing**: Finally, you'll need to test your program thoroughly. This could involve creating graphs by hand, running them through your program, and checking the results against expected outcomes.\\n\\nHowever, it's important to note that assembly language is not typically used for implementing complex algorithms like Dijkstra's due to its low-level nature and the complexity of managing memory and pointers. High-level languages like Python or C++ are usually better suited for this kind of task. If you're interested in learning more about assembly language, it might be better to start with simpler tasks before moving on to something as complex as Dijkstra's algorithm.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"mixtral:8x7b\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is most efficient way to solve Dijkstra's algorithm in win32 assembly?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the JSON Output Parser\n",
    "\n",
    "The JSON Output Parser allows us to take the result of a model call and parse it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```\\nTell me a joke.\\n')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"mixtral:8x7b\")\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt #| model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"setup\": \"Why don't scientists trust atoms?\",\n",
      "  \"punchline\": \"Because they make up everything!\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model #| parser\n",
    "\n",
    "result = chain.invoke({\"query\": joke_query})\n",
    "\n",
    "print(result)\n",
    "\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': \"Why don't scientists trust atoms?\", 'punchline': 'Because they make up everything!'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "result = chain.invoke({\"query\": joke_query})\n",
    "\n",
    "print(result)\n",
    "\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Parser\n",
    "\n",
    "It is possible that the model, despite the best efforts of the prompt engineering, does not produce valid JSON. It could also fail due to a timeout. Retry Parsing allows you to specify a max number of additional calls to make to the model to retry generating valid JSON.\n",
    "\n",
    "The retry logic does a bit more than just a retry, it passes in the prompt (as well as the original output) to try again to get a better response.\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/retry/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
